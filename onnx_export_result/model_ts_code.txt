
Code for root model, type=ScriptableAdapter:
class ScriptableAdapter(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  model : __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN
  training : Final[bool] = False
  def forward(self: __torch__.ScriptableAdapter,
    inputs: Tuple[Dict[str, Tensor]]) -> List[Dict[str, Tensor]]:
    model = self.model
    _0, = inputs
    instances = (model).inference([_0], None, False, )
    _1 = annotate(List[Dict[str, Tensor]], [])
    for _2 in range(torch.len(instances)):
      i = instances[_2]
      _3 = torch.append(_1, (i).get_fields())
    return _1

--------------------------------------------------------------------------------
Code for .model, type=GeneralizedRCNN:
class GeneralizedRCNN(Module):
  __parameters__ = []
  __buffers__ = ["pixel_mean", "pixel_std", ]
  pixel_mean : Tensor
  pixel_std : Tensor
  _is_full_backward_hook : Optional[bool]
  input_format : str
  vis_period : int
  backbone : __torch__.detectron2.modeling.backbone.fpn.FPN
  proposal_generator : __torch__.detectron2.modeling.proposal_generator.rpn.RPN
  roi_heads : __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads
  training : Final[bool] = False
  def __device_getter(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN) -> Device:
    pixel_mean = self.pixel_mean
    return ops.prim.device(pixel_mean)
  def forward(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _0 = (self).inference(batched_inputs, None, True, )
    return _0
  def inference(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]],
    detected_instances: Optional[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]]=None,
    do_postprocess: bool=True) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _1 = "AssertionError: Scripting is not supported for postprocess."
    images = (self).preprocess_image(batched_inputs, )
    backbone = self.backbone
    tensor = images.tensor
    features = (backbone).forward(tensor, )
    _2 = torch.__is__(detected_instances, None)
    if _2:
      proposal_generator = self.proposal_generator
      _3 = (proposal_generator).forward(images, features, None, )
      proposals, _4, = _3
      roi_heads = self.roi_heads
      _5 = (roi_heads).forward(images, features, proposals, None, )
      results0, _6, = _5
      results = results0
    else:
      detected_instances0 = unchecked_cast(List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], detected_instances)
      detected_instances1 = annotate(List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], [])
      for _7 in range(torch.len(detected_instances0)):
        x = detected_instances0[_7]
        _8 = (x).to((self).__device_getter(), )
        _9 = torch.append(detected_instances1, _8)
      roi_heads0 = self.roi_heads
      results1 = (roi_heads0).forward_with_given_boxes(features, detected_instances1, )
      results = results1
    if do_postprocess:
      ops.prim.RaiseException(_1)
    else:
      pass
    return results
  def preprocess_image(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]]) -> __torch__.detectron2.structures.image_list.ImageList:
    _10 = __torch__.detectron2.structures.image_list.from_tensors
    images = annotate(List[Tensor], [])
    for _11 in range(torch.len(batched_inputs)):
      x = batched_inputs[_11]
      _12 = (self)._move_to_current_device(x["image"], )
      _13 = torch.append(images, _12)
    images0 = annotate(List[Tensor], [])
    for _14 in range(torch.len(images)):
      x0 = images[_14]
      pixel_mean = self.pixel_mean
      _15 = torch.sub(x0, pixel_mean)
      pixel_std = self.pixel_std
      _16 = torch.append(images0, torch.div(_15, pixel_std))
    backbone = self.backbone
    _17 = (backbone).__size_divisibility_getter()
    backbone0 = self.backbone
    _18 = (backbone0).__padding_constraints_getter()
    return _10(images0, _17, 0., _18, )
  def _move_to_current_device(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    x: Tensor) -> Tensor:
    _19 = __torch__.detectron2.layers.wrappers.move_device_like
    pixel_mean = self.pixel_mean
    return _19(x, pixel_mean, )

--------------------------------------------------------------------------------
Code for .model.backbone, type=FPN:
class FPN(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  in_features : Tuple[str, str, str, str]
  _out_feature_strides : Dict[str, int]
  _out_features : List[str]
  _out_feature_channels : Dict[str, int]
  _size_divisibility : int
  _square_pad : int
  top_block : __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool
  bottom_up : __torch__.detectron2.modeling.backbone.resnet.ResNet
  lateral_convs : __torch__.torch.nn.modules.container.___torch_mangle_33.ModuleList
  output_convs : __torch__.torch.nn.modules.container.___torch_mangle_35.ModuleList
  training : Final[bool] = False
  _fuse_type : Final[str] = "sum"
  def __padding_constraints_getter(self: __torch__.detectron2.modeling.backbone.fpn.FPN) -> Dict[str, int]:
    _square_pad = self._square_pad
    return {"square_size": _square_pad}
  def __size_divisibility_getter(self: __torch__.detectron2.modeling.backbone.fpn.FPN) -> int:
    return self._size_divisibility
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.FPN,
    x: Tensor) -> Dict[str, Tensor]:
    _0 = __torch__.torch.nn.functional.___torch_mangle_44.interpolate
    bottom_up = self.bottom_up
    bottom_up_features = (bottom_up).forward(x, )
    _1 = annotate(List[Tensor], [])
    lateral_convs = self.lateral_convs
    _00 = getattr(lateral_convs, "0")
    in_features = self.in_features
    _2 = bottom_up_features[(in_features)[-1]]
    prev_features = (_00).forward(_2, )
    output_convs = self.output_convs
    _01 = getattr(output_convs, "0")
    _3 = torch.append(_1, (_01).forward(prev_features, ))
    lateral_convs0 = self.lateral_convs
    _10 = getattr(lateral_convs0, "1")
    _20 = getattr(lateral_convs0, "2")
    _30 = getattr(lateral_convs0, "3")
    output_convs0 = self.output_convs
    _11 = getattr(output_convs0, "1")
    _21 = getattr(output_convs0, "2")
    _31 = getattr(output_convs0, "3")
    in_features0 = self.in_features
    features = (in_features0)[-2]
    features0 = bottom_up_features[features]
    top_down_features = _0(prev_features, None, 2., "nearest", None, None, False, )
    lateral_features = (_10).forward(features0, )
    prev_features0 = torch.add(lateral_features, top_down_features)
    torch.insert(_1, 0, (_11).forward(prev_features0, ))
    in_features1 = self.in_features
    features1 = (in_features1)[-3]
    features2 = bottom_up_features[features1]
    top_down_features0 = _0(prev_features0, None, 2., "nearest", None, None, False, )
    lateral_features0 = (_20).forward(features2, )
    prev_features1 = torch.add(lateral_features0, top_down_features0)
    torch.insert(_1, 0, (_21).forward(prev_features1, ))
    in_features2 = self.in_features
    features3 = (in_features2)[-4]
    features4 = bottom_up_features[features3]
    top_down_features1 = _0(prev_features1, None, 2., "nearest", None, None, False, )
    lateral_features1 = (_30).forward(features4, )
    prev_features2 = torch.add(lateral_features1, top_down_features1)
    torch.insert(_1, 0, (_31).forward(prev_features2, ))
    top_block = self.top_block
    in_feature = top_block.in_feature
    _4 = torch.__contains__(bottom_up_features, in_feature)
    if _4:
      top_block0 = self.top_block
      in_feature0 = top_block0.in_feature
      top_block_in_feature = bottom_up_features[in_feature0]
    else:
      _out_features = self._out_features
      top_block1 = self.top_block
      in_feature1 = top_block1.in_feature
      _5 = torch.index(_out_features, in_feature1)
      top_block_in_feature = _1[_5]
    top_block2 = self.top_block
    _6 = (top_block2).forward(top_block_in_feature, )
    torch.extend(_1, _6)
    _out_features0 = self._out_features
    _7 = torch.eq(torch.len(_out_features0), torch.len(_1))
    if _7:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    _8 = annotate(Dict[str, Tensor], {})
    _out_features1 = self._out_features
    _9 = [torch.len(_out_features1), torch.len(_1)]
    for _12 in range(ops.prim.min(_9)):
      f = _out_features1[_12]
      res = _1[_12]
      torch._set_item(_8, f, res)
    return _8

--------------------------------------------------------------------------------
Code for .model.backbone.top_block, type=LastLevelMaxPool:
class LastLevelMaxPool(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_levels : int
  in_feature : str
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool,
    x: Tensor) -> List[Tensor]:
    _0 = __torch__.torch.nn.functional._max_pool2d
    _1 = _0(x, [1, 1], [2, 2], [0, 0], [1, 1], False, False, )
    return [_1]

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up, type=ResNet:
class ResNet(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_classes : NoneType
  _out_feature_strides : Dict[str, int]
  _out_feature_channels : Dict[str, int]
  stage_names : Tuple[str, str, str, str]
  _out_features : List[str]
  stem : __torch__.detectron2.modeling.backbone.resnet.BasicStem
  stages : __torch__.torch.nn.modules.container.ModuleList
  training : Final[bool] = False
  def __padding_constraints_getter(self: __torch__.detectron2.modeling.backbone.resnet.ResNet) -> Dict[str, int]:
    return annotate(Dict[str, int], {})
  def __size_divisibility_getter(self: __torch__.detectron2.modeling.backbone.resnet.ResNet) -> int:
    return 0
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.ResNet,
    x: Tensor) -> Dict[str, Tensor]:
    _0 = "ResNet takes an input of shape (N, C, H, W). Got {} instead!"
    if torch.eq(torch.dim(x), 4):
      pass
    else:
      _1 = torch.add("AssertionError: ", torch.format(_0, torch.size(x)))
      ops.prim.RaiseException(_1)
    outputs = annotate(Dict[str, Tensor], {})
    stem = self.stem
    x0 = (stem).forward(x, )
    _out_features = self._out_features
    _2 = torch.__contains__(_out_features, "stem")
    if _2:
      torch._set_item(outputs, "stem", x0)
    else:
      pass
    stage_names = self.stage_names
    name, name0, name1, name2, = stage_names
    stages = self.stages
    _00 = getattr(stages, "0")
    _10 = getattr(stages, "1")
    _20 = getattr(stages, "2")
    _3 = getattr(stages, "3")
    x1 = (_00).forward(x0, )
    _out_features0 = self._out_features
    _4 = torch.__contains__(_out_features0, name)
    if _4:
      torch._set_item(outputs, name, x1)
    else:
      pass
    x2 = (_10).forward(x1, )
    _out_features1 = self._out_features
    _5 = torch.__contains__(_out_features1, name0)
    if _5:
      torch._set_item(outputs, name0, x2)
    else:
      pass
    x3 = (_20).forward(x2, )
    _out_features2 = self._out_features
    _6 = torch.__contains__(_out_features2, name1)
    if _6:
      torch._set_item(outputs, name1, x3)
    else:
      pass
    x4 = (_3).forward(x3, )
    _out_features3 = self._out_features
    _7 = torch.__contains__(_out_features3, name2)
    if _7:
      torch._set_item(outputs, name2, x4)
    else:
      pass
    return outputs

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem, type=BasicStem:
class BasicStem(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  conv1 : __torch__.detectron2.layers.wrappers.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BasicStem,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional._max_pool2d
    conv1 = self.conv1
    x0 = (conv1).forward(x, )
    x1 = torch.relu_(x0)
    x2 = _0(x1, [3, 3], [2, 2], [1, 1], [1, 1], False, False, )
    return x2

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (7, 7)
  padding : Final[Tuple[int, int]] = (3, 3)
  in_channels : Final[int] = 3
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [3, 3], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.torch.nn.modules.container.Sequential
  __annotations__["1"] = __torch__.torch.nn.modules.container.___torch_mangle_12.Sequential
  __annotations__["2"] = __torch__.torch.nn.modules.container.___torch_mangle_20.Sequential
  __annotations__["3"] = __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential
  training : Final[bool] = True
  def __len__(self: __torch__.torch.nn.modules.container.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    return (_2).forward(input1, )
  def __len__(self: __torch__.torch.nn.modules.container.Sequential) -> int:
    return 3

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 64
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_9.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_12.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    _3 = getattr(self, "3")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    input2 = (_2).forward(input1, )
    return (_3).forward(input2, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_12.Sequential) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_9.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 128
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_17.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["4"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["5"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_20.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    _3 = getattr(self, "3")
    _4 = getattr(self, "4")
    _5 = getattr(self, "5")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    input2 = (_2).forward(input1, )
    input3 = (_3).forward(input2, )
    input4 = (_4).forward(input3, )
    return (_5).forward(input4, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_20.Sequential) -> int:
    return 6

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_14.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_17.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_14.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_25.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    return (_2).forward(input1, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential) -> int:
    return 3

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_22.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_25.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 2048
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_22.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 2048
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 2048
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 2048
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 2048
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 2048
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", "num_batches_tracked", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  num_batches_tracked : NoneType
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.wrappers.___torch_mangle_29.Conv2d
  __annotations__["1"] = __torch__.detectron2.layers.wrappers.___torch_mangle_30.Conv2d
  __annotations__["2"] = __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d
  __annotations__["3"] = __torch__.detectron2.layers.wrappers.___torch_mangle_32.Conv2d
  training : Final[bool] = True
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_33.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.0, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 2048
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_29.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 1024
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_30.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 512
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_32.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  __annotations__["1"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  __annotations__["2"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  __annotations__["3"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  training : Final[bool] = True
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_35.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.0, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.proposal_generator, type=RPN:
class RPN(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_features : List[str]
  anchor_matcher : __torch__.detectron2.modeling.matcher.Matcher
  box2box_transform : __torch__.detectron2.modeling.box_regression.Box2BoxTransform
  batch_size_per_image : int
  positive_fraction : float
  pre_nms_topk : Dict[bool, int]
  post_nms_topk : Dict[bool, int]
  nms_thresh : float
  min_box_size : float
  anchor_boundary_thresh : int
  loss_weight : Dict[str, float]
  box_reg_loss_type : str
  smooth_l1_beta : float
  rpn_head : __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead
  anchor_generator : __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    images: __torch__.detectron2.structures.image_list.ImageList,
    features: Dict[str, Tensor],
    gt_instances: Optional[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]]=None) -> Tuple[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], Dict[str, Tensor]]:
    features0 = annotate(List[Tensor], [])
    in_features = self.in_features
    for _0 in range(torch.len(in_features)):
      f = in_features[_0]
      _1 = torch.append(features0, features[f])
    anchor_generator = self.anchor_generator
    anchors = (anchor_generator).forward(features0, )
    rpn_head = self.rpn_head
    pred_objectness_logits, pred_anchor_deltas, = (rpn_head).forward(features0, )
    pred_objectness_logits0 = annotate(List[Tensor], [])
    for _2 in range(torch.len(pred_objectness_logits)):
      score = pred_objectness_logits[_2]
      _3 = torch.permute(score, [0, 2, 3, 1])
      _4 = torch.append(pred_objectness_logits0, torch.flatten(_3, 1))
    pred_anchor_deltas0 = annotate(List[Tensor], [])
    for _5 in range(torch.len(pred_anchor_deltas)):
      x = pred_anchor_deltas[_5]
      _6 = [(torch.size(x))[0], -1, 4, (torch.size(x))[-2], (torch.size(x))[-1]]
      _7 = torch.permute(torch.view(x, _6), [0, 3, 4, 1, 2])
      _8 = torch.append(pred_anchor_deltas0, torch.flatten(_7, 1, -2))
    losses = annotate(Dict[str, Tensor], {})
    image_sizes = images.image_sizes
    proposals = (self).predict_proposals(anchors, pred_objectness_logits0, pred_anchor_deltas0, image_sizes, )
    return (proposals, losses)
  def predict_proposals(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    anchors: List[__torch__.detectron2.structures.boxes.Boxes],
    pred_objectness_logits: List[Tensor],
    pred_anchor_deltas: List[Tensor],
    image_sizes: List[Tuple[int, int]]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _9 = __torch__.detectron2.modeling.proposal_generator.proposal_utils.find_top_rpn_proposals
    _10 = __torch__.torch.autograd.grad_mode.no_grad.__new__(__torch__.torch.autograd.grad_mode.no_grad)
    _11 = (_10).__init__()
    with _10:
      pred_proposals = (self)._decode_proposals(anchors, pred_anchor_deltas, )
      nms_thresh = self.nms_thresh
      pre_nms_topk = self.pre_nms_topk
      _12 = pre_nms_topk[False]
      post_nms_topk = self.post_nms_topk
      _13 = post_nms_topk[False]
      min_box_size = self.min_box_size
      _14 = _9(pred_proposals, pred_objectness_logits, image_sizes, nms_thresh, _12, _13, min_box_size, False, )
    return _14
  def _decode_proposals(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    anchors: List[__torch__.detectron2.structures.boxes.Boxes],
    pred_anchor_deltas: List[Tensor]) -> List[Tensor]:
    N = (torch.size(pred_anchor_deltas[0]))[0]
    _15 = annotate(List[Tensor], [])
    _16 = [torch.len(anchors), torch.len(pred_anchor_deltas)]
    for _17 in range(ops.prim.min(_16)):
      anchors_i = anchors[_17]
      pred_anchor_deltas_i = pred_anchor_deltas[_17]
      tensor = anchors_i.tensor
      B = torch.size(tensor, 1)
      pred_anchor_deltas_i0 = torch.reshape(pred_anchor_deltas_i, [-1, B])
      tensor0 = anchors_i.tensor
      _18 = torch.expand(torch.unsqueeze(tensor0, 0), [N, -1, -1])
      anchors_i0 = torch.reshape(_18, [-1, B])
      box2box_transform = self.box2box_transform
      proposals_i = (box2box_transform).apply_deltas(pred_anchor_deltas_i0, anchors_i0, )
      _19 = torch.view(proposals_i, [N, -1, B])
      _20 = torch.append(_15, _19)
    return _15

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head, type=StandardRPNHead:
class StandardRPNHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d
  objectness_logits : __torch__.torch.nn.modules.conv.Conv2d
  anchor_deltas : __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead,
    features: List[Tensor]) -> Tuple[List[Tensor], List[Tensor]]:
    _0 = annotate(List[Tensor], [])
    _1 = annotate(List[Tensor], [])
    for _2 in range(torch.len(features)):
      x = features[_2]
      conv = self.conv
      t = (conv).forward(x, )
      objectness_logits = self.objectness_logits
      _3 = torch.append(_0, (objectness_logits).forward(t, ))
      anchor_deltas = self.anchor_deltas
      _4 = torch.append(_1, (anchor_deltas).forward(t, ))
    return (_0, _1)

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : __torch__.torch.nn.modules.activation.ReLU
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    activation = self.activation
    return (activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.objectness_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 3
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.conv.Conv2d,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    _0 = (self)._conv_forward(input, weight, bias, )
    return _0
  def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv2d,
    input: Tensor,
    weight: Tensor,
    bias: Optional[Tensor]) -> Tensor:
    _1 = torch.conv2d(input, weight, bias, [1, 1], [0, 0], [1, 1])
    return _1

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.anchor_deltas, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 12
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    _0 = (self)._conv_forward(input, weight, bias, )
    return _0
  def _conv_forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d,
    input: Tensor,
    weight: Tensor,
    bias: Optional[Tensor]) -> Tensor:
    _1 = torch.conv2d(input, weight, bias, [1, 1], [0, 0], [1, 1])
    return _1

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator, type=DefaultAnchorGenerator:
class DefaultAnchorGenerator(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  strides : List[int]
  num_features : int
  offset : float
  cell_anchors : __torch__.detectron2.modeling.anchor_generator.BufferList
  training : Final[bool] = False
  box_dim : Final[int] = 4
  def forward(self: __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator,
    features: List[Tensor]) -> List[__torch__.detectron2.structures.boxes.Boxes]:
    grid_sizes = annotate(List[List[int]], [])
    for _0 in range(torch.len(features)):
      feature_map = features[_0]
      _1 = torch.slice(torch.size(feature_map), -2)
      _2 = torch.append(grid_sizes, _1)
    anchors_over_all_feature_maps = (self)._grid_anchors(grid_sizes, )
    _3 = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    _4 = torch.len(anchors_over_all_feature_maps)
    for _5 in range(_4):
      x = anchors_over_all_feature_maps[_5]
      _6 = __torch__.detectron2.structures.boxes.Boxes.__new__(__torch__.detectron2.structures.boxes.Boxes)
      _7 = (_6).__init__(x, )
      _8 = torch.append(_3, _6)
    return _3
  def _grid_anchors(self: __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator,
    grid_sizes: List[List[int]]) -> List[Tensor]:
    _9 = __torch__.detectron2.modeling.anchor_generator._create_grid_offsets
    _10 = annotate(List[Tensor], [])
    buffers = annotate(List[Tensor], [])
    cell_anchors = self.cell_anchors
    _0 = getattr(cell_anchors, "0")
    _1 = getattr(cell_anchors, "1")
    _2 = getattr(cell_anchors, "2")
    _3 = getattr(cell_anchors, "3")
    _4 = getattr(cell_anchors, "4")
    _11 = torch.append(buffers, _0)
    _12 = torch.append(buffers, _1)
    _13 = torch.append(buffers, _2)
    _14 = torch.append(buffers, _3)
    _15 = torch.append(buffers, _4)
    strides = self.strides
    _16 = [torch.len(grid_sizes), torch.len(strides), torch.len(buffers)]
    for _17 in range(ops.prim.min(_16)):
      size = grid_sizes[_17]
      stride = strides[_17]
      base_anchors = buffers[_17]
      offset = self.offset
      _18 = _9(size, stride, offset, base_anchors, )
      shift_x, shift_y, = _18
      _19 = [shift_x, shift_y, shift_x, shift_y]
      shifts = torch.stack(_19, 1)
      _20 = torch.view(shifts, [-1, 1, 4])
      _21 = torch.view(base_anchors, [1, -1, 4])
      _22 = torch.reshape(torch.add(_20, _21), [-1, 4])
      _23 = torch.append(_10, _22)
    return _10

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator.cell_anchors, type=BufferList:
class BufferList(Module):
  __parameters__ = []
  __buffers__ = ["0", "1", "2", "3", "4", ]
  __annotations__ = []
  __annotations__["0"] = Tensor
  __annotations__["1"] = Tensor
  __annotations__["2"] = Tensor
  __annotations__["3"] = Tensor
  __annotations__["4"] = Tensor
  _is_full_backward_hook : Optional[bool]
  training : Final[bool] = False

--------------------------------------------------------------------------------
Code for .model.roi_heads, type=StandardROIHeads:
class StandardROIHeads(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  batch_size_per_image : int
  positive_fraction : float
  num_classes : int
  proposal_matcher : __torch__.detectron2.modeling.matcher.Matcher
  proposal_append_gt : bool
  in_features : List[str]
  box_in_features : List[str]
  mask_in_features : List[str]
  train_on_pred_boxes : bool
  box_pooler : __torch__.detectron2.modeling.poolers.ROIPooler
  box_head : __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead
  box_predictor : __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers
  mask_pooler : __torch__.detectron2.modeling.poolers.ROIPooler
  mask_head : __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead
  training : Final[bool] = False
  keypoint_on : Final[bool] = False
  mask_on : Final[bool] = True
  def forward(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    images: __torch__.detectron2.structures.image_list.ImageList,
    features: Dict[str, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1],
    targets: Optional[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]]=None) -> Tuple[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], Dict[str, Tensor]]:
    pred_instances = (self)._forward_box(features, proposals, )
    pred_instances0 = (self).forward_with_given_boxes(features, pred_instances, )
    _0 = (pred_instances0, annotate(Dict[str, Tensor], {}))
    return _0
  def _forward_box(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    features0 = annotate(List[Tensor], [])
    box_in_features = self.box_in_features
    for _1 in range(torch.len(box_in_features)):
      f = box_in_features[_1]
      _2 = torch.append(features0, features[f])
    box_pooler = self.box_pooler
    _3 = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    for _4 in range(torch.len(proposals)):
      x = proposals[_4]
      _5 = torch.append(_3, (x).__proposal_boxes_getter())
    box_features = (box_pooler).forward(features0, _3, )
    box_head = self.box_head
    box_features0 = (box_head).forward(box_features, )
    box_predictor = self.box_predictor
    predictions = (box_predictor).forward(box_features0, )
    box_predictor0 = self.box_predictor
    _6 = (box_predictor0).inference(predictions, proposals, )
    pred_instances, _7, = _6
    return pred_instances
  def forward_with_given_boxes(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    if (instances[0]).has("pred_boxes", ):
      _9 = (instances[0]).has("pred_classes", )
      _8 = _9
    else:
      _8 = False
    if _8:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    instances0 = (self)._forward_mask(features, instances, )
    instances1 = (self)._forward_keypoint(features, instances0, )
    return instances1
  def _forward_mask(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    features1 = annotate(List[Tensor], [])
    mask_in_features = self.mask_in_features
    for _10 in range(torch.len(mask_in_features)):
      f = mask_in_features[_10]
      _11 = torch.append(features1, features[f])
    boxes = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    for _12 in range(torch.len(instances)):
      x = instances[_12]
      _13 = torch.append(boxes, (x).__pred_boxes_getter())
    mask_pooler = self.mask_pooler
    features2 = (mask_pooler).forward(features1, boxes, )
    mask_head = self.mask_head
    _14 = (mask_head).forward(features2, instances, )
    return _14
  def _forward_keypoint(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    return instances

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  min_level : int
  max_level : int
  canonical_level : int
  canonical_box_size : int
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.poolers.ROIPooler,
    x: List[Tensor],
    box_lists: List[__torch__.detectron2.structures.boxes.Boxes]) -> Tensor:
    _0 = __torch__.detectron2.utils.tracing.is_fx_tracing
    _1 = "unequal value, num_level_assignments={}, but x is list of {} Tensors"
    _2 = __torch__.detectron2.utils.tracing.assert_fx_safe
    _3 = "unequal value, x[0] batch dim 0 is {}, but box_list has length {}"
    _4 = __torch__.detectron2.modeling.poolers._create_zeros
    _5 = __torch__.detectron2.modeling.poolers.convert_boxes_to_pooler_format
    _6 = __torch__.detectron2.modeling.poolers.assign_boxes_to_levels
    _7 = __torch__.detectron2.layers.wrappers.nonzero_tuple
    level_poolers = self.level_poolers
    num_level_assignments = (level_poolers).__len__()
    if torch.__not__(_0()):
      x0 = unchecked_cast(List[Tensor], x)
    else:
      x0 = x
    _8 = torch.eq(torch.len(x0), num_level_assignments)
    _9 = torch.format(_1, num_level_assignments, torch.len(x0))
    _10 = _2(_8, _9, )
    _11 = torch.eq(torch.len(box_lists), torch.size(x0[0], 0))
    _12 = torch.format(_3, torch.size(x0[0], 0), torch.len(box_lists))
    _13 = _2(_11, _12, )
    if torch.eq(torch.len(box_lists), 0):
      _15 = (torch.size(x0[0]))[1]
      output_size = self.output_size
      _16, _17, = output_size
      _14 = _4(None, _15, _16, _17, x0[0], )
    else:
      pooler_fmt_boxes = _5(box_lists, )
      _18 = torch.eq(num_level_assignments, 1)
      if _18:
        level_poolers0 = self.level_poolers
        _00 = getattr(level_poolers0, "0")
        _20 = (_00).forward(x0[0], pooler_fmt_boxes, )
        _19 = _20
      else:
        min_level = self.min_level
        max_level = self.max_level
        canonical_box_size = self.canonical_box_size
        canonical_level = self.canonical_level
        level_assignments = _6(box_lists, min_level, max_level, canonical_box_size, canonical_level, )
        num_channels = (torch.size(x0[0]))[1]
        output_size0 = self.output_size
        output_size1 = (output_size0)[0]
        output = _4(pooler_fmt_boxes, num_channels, output_size1, output_size1, x0[0], )
        level_poolers1 = self.level_poolers
        _01 = getattr(level_poolers1, "0")
        _110 = getattr(level_poolers1, "1")
        _21 = getattr(level_poolers1, "2")
        _30 = getattr(level_poolers1, "3")
        _22 = _7(torch.eq(level_assignments, 0), )
        inds = _22[0]
        _23 = annotate(List[Optional[Tensor]], [inds])
        pooler_fmt_boxes_level = torch.index(pooler_fmt_boxes, _23)
        _24 = (_01).forward(x0[0], pooler_fmt_boxes_level, )
        _25 = annotate(List[Optional[Tensor]], [inds])
        _26 = torch.index_put_(output, _25, _24)
        _27 = _7(torch.eq(level_assignments, 1), )
        inds0 = _27[0]
        _28 = annotate(List[Optional[Tensor]], [inds0])
        pooler_fmt_boxes_level0 = torch.index(pooler_fmt_boxes, _28)
        _29 = (_110).forward(x0[1], pooler_fmt_boxes_level0, )
        _31 = annotate(List[Optional[Tensor]], [inds0])
        _32 = torch.index_put_(output, _31, _29)
        _33 = _7(torch.eq(level_assignments, 2), )
        inds1 = _33[0]
        _34 = annotate(List[Optional[Tensor]], [inds1])
        pooler_fmt_boxes_level1 = torch.index(pooler_fmt_boxes, _34)
        _35 = (_21).forward(x0[2], pooler_fmt_boxes_level1, )
        _36 = annotate(List[Optional[Tensor]], [inds1])
        _37 = torch.index_put_(output, _36, _35)
        _38 = _7(torch.eq(level_assignments, 3), )
        inds2 = _38[0]
        _39 = annotate(List[Optional[Tensor]], [inds2])
        pooler_fmt_boxes_level2 = torch.index(pooler_fmt_boxes, _39)
        _40 = (_30).forward(x0[3], pooler_fmt_boxes_level2, )
        _41 = annotate(List[Optional[Tensor]], [inds2])
        _42 = torch.index_put_(output, _41, _40)
        _19 = output
      _14 = _19
    return _14

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.ROIAlign
  training : Final[bool] = False
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head, type=FastRCNNConvFCHead:
class FastRCNNConvFCHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  _output_size : int
  flatten : __torch__.torch.nn.modules.flatten.Flatten
  fc1 : __torch__.torch.nn.modules.linear.Linear
  fc_relu1 : __torch__.torch.nn.modules.activation.ReLU
  fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear
  fc_relu2 : __torch__.torch.nn.modules.activation.ReLU
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead,
    x: Tensor) -> Tensor:
    flatten = self.flatten
    fc1 = self.fc1
    fc_relu1 = self.fc_relu1
    fc2 = self.fc2
    fc_relu2 = self.fc_relu2
    x0 = (flatten).forward(x, )
    x1 = (fc1).forward(x0, )
    x2 = (fc_relu1).forward(x1, )
    x3 = (fc2).forward(x2, )
    return (fc_relu2).forward(x3, )
  def __len__(self: __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead) -> int:
    return 5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.flatten, type=Flatten:
class Flatten(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  start_dim : Final[int] = 1
  end_dim : Final[int] = -1
  def forward(self: __torch__.torch.nn.modules.flatten.Flatten,
    input: Tensor) -> Tensor:
    return torch.flatten(input, 1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc1, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 12544
  out_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu1, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc2, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 1024
  out_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu2, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor, type=FastRCNNOutputLayers:
class FastRCNNOutputLayers(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_classes : int
  box2box_transform : __torch__.detectron2.modeling.box_regression.Box2BoxTransform
  smooth_l1_beta : float
  test_score_thresh : float
  test_nms_thresh : float
  test_topk_per_image : int
  box_reg_loss_type : str
  loss_weight : Dict[str, float]
  use_fed_loss : bool
  use_sigmoid_ce : bool
  fed_loss_num_classes : int
  cls_score : __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear
  bbox_pred : __torch__.torch.nn.modules.linear.___torch_mangle_41.Linear
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    x: Tensor) -> Tuple[Tensor, Tensor]:
    if torch.gt(torch.dim(x), 2):
      x0 = torch.flatten(x, 1)
    else:
      x0 = x
    cls_score = self.cls_score
    scores = (cls_score).forward(x0, )
    bbox_pred = self.bbox_pred
    proposal_deltas = (bbox_pred).forward(x0, )
    return (scores, proposal_deltas)
  def inference(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> Tuple[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], List[Tensor]]:
    _0 = __torch__.detectron2.modeling.roi_heads.fast_rcnn.fast_rcnn_inference
    boxes = (self).predict_boxes(predictions, proposals, )
    scores = (self).predict_probs(predictions, proposals, )
    image_shapes = annotate(List[Tuple[int, int]], [])
    for _1 in range(torch.len(proposals)):
      x = proposals[_1]
      image_size = x.image_size
      _2 = torch.append(image_shapes, image_size)
    test_score_thresh = self.test_score_thresh
    test_nms_thresh = self.test_nms_thresh
    test_topk_per_image = self.test_topk_per_image
    _3 = _0(boxes, scores, image_shapes, test_score_thresh, test_nms_thresh, test_topk_per_image, )
    return _3
  def predict_boxes(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[Tensor]:
    _4 = __torch__.detectron2.layers.wrappers.cat
    _5 = torch.__not__(bool(torch.len(proposals)))
    if _5:
      _6 = annotate(List[Tensor], [])
    else:
      _7, proposal_deltas, = predictions
      num_prop_per_image = annotate(List[int], [])
      for _8 in range(torch.len(proposals)):
        p = proposals[_8]
        _9 = torch.append(num_prop_per_image, (p).__len__())
      _10 = annotate(List[Tensor], [])
      for _11 in range(torch.len(proposals)):
        p0 = proposals[_11]
        tensor = (p0).__proposal_boxes_getter().tensor
        _12 = torch.append(_10, tensor)
      proposal_boxes = _4(_10, 0, )
      box2box_transform = self.box2box_transform
      predict_boxes = (box2box_transform).apply_deltas(proposal_deltas, proposal_boxes, )
      _13 = torch.split(predict_boxes, num_prop_per_image)
      _6 = _13
    return _6
  def predict_probs(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[Tensor]:
    _14 = __torch__.torch.nn.functional.softmax
    scores, _15, = predictions
    num_inst_per_image = annotate(List[int], [])
    for _16 in range(torch.len(proposals)):
      p = proposals[_16]
      _17 = torch.append(num_inst_per_image, (p).__len__())
    use_sigmoid_ce = self.use_sigmoid_ce
    if use_sigmoid_ce:
      probs = torch.sigmoid(scores)
    else:
      probs = _14(scores, -1, 3, None, )
    _18 = torch.split(probs, num_inst_per_image)
    return _18

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.cls_score, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 1024
  out_features : Final[int] = 15
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.bbox_pred, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 1024
  out_features : Final[int] = 56
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_41.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  min_level : int
  max_level : int
  canonical_level : int
  canonical_box_size : int
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.poolers.ROIPooler,
    x: List[Tensor],
    box_lists: List[__torch__.detectron2.structures.boxes.Boxes]) -> Tensor:
    _0 = __torch__.detectron2.utils.tracing.is_fx_tracing
    _1 = "unequal value, num_level_assignments={}, but x is list of {} Tensors"
    _2 = __torch__.detectron2.utils.tracing.assert_fx_safe
    _3 = "unequal value, x[0] batch dim 0 is {}, but box_list has length {}"
    _4 = __torch__.detectron2.modeling.poolers._create_zeros
    _5 = __torch__.detectron2.modeling.poolers.convert_boxes_to_pooler_format
    _6 = __torch__.detectron2.modeling.poolers.assign_boxes_to_levels
    _7 = __torch__.detectron2.layers.wrappers.nonzero_tuple
    level_poolers = self.level_poolers
    num_level_assignments = (level_poolers).__len__()
    if torch.__not__(_0()):
      x0 = unchecked_cast(List[Tensor], x)
    else:
      x0 = x
    _8 = torch.eq(torch.len(x0), num_level_assignments)
    _9 = torch.format(_1, num_level_assignments, torch.len(x0))
    _10 = _2(_8, _9, )
    _11 = torch.eq(torch.len(box_lists), torch.size(x0[0], 0))
    _12 = torch.format(_3, torch.size(x0[0], 0), torch.len(box_lists))
    _13 = _2(_11, _12, )
    if torch.eq(torch.len(box_lists), 0):
      _15 = (torch.size(x0[0]))[1]
      output_size = self.output_size
      _16, _17, = output_size
      _14 = _4(None, _15, _16, _17, x0[0], )
    else:
      pooler_fmt_boxes = _5(box_lists, )
      _18 = torch.eq(num_level_assignments, 1)
      if _18:
        level_poolers0 = self.level_poolers
        _00 = getattr(level_poolers0, "0")
        _20 = (_00).forward(x0[0], pooler_fmt_boxes, )
        _19 = _20
      else:
        min_level = self.min_level
        max_level = self.max_level
        canonical_box_size = self.canonical_box_size
        canonical_level = self.canonical_level
        level_assignments = _6(box_lists, min_level, max_level, canonical_box_size, canonical_level, )
        num_channels = (torch.size(x0[0]))[1]
        output_size0 = self.output_size
        output_size1 = (output_size0)[0]
        output = _4(pooler_fmt_boxes, num_channels, output_size1, output_size1, x0[0], )
        level_poolers1 = self.level_poolers
        _01 = getattr(level_poolers1, "0")
        _110 = getattr(level_poolers1, "1")
        _21 = getattr(level_poolers1, "2")
        _30 = getattr(level_poolers1, "3")
        _22 = _7(torch.eq(level_assignments, 0), )
        inds = _22[0]
        _23 = annotate(List[Optional[Tensor]], [inds])
        pooler_fmt_boxes_level = torch.index(pooler_fmt_boxes, _23)
        _24 = (_01).forward(x0[0], pooler_fmt_boxes_level, )
        _25 = annotate(List[Optional[Tensor]], [inds])
        _26 = torch.index_put_(output, _25, _24)
        _27 = _7(torch.eq(level_assignments, 1), )
        inds0 = _27[0]
        _28 = annotate(List[Optional[Tensor]], [inds0])
        pooler_fmt_boxes_level0 = torch.index(pooler_fmt_boxes, _28)
        _29 = (_110).forward(x0[1], pooler_fmt_boxes_level0, )
        _31 = annotate(List[Optional[Tensor]], [inds0])
        _32 = torch.index_put_(output, _31, _29)
        _33 = _7(torch.eq(level_assignments, 2), )
        inds1 = _33[0]
        _34 = annotate(List[Optional[Tensor]], [inds1])
        pooler_fmt_boxes_level1 = torch.index(pooler_fmt_boxes, _34)
        _35 = (_21).forward(x0[2], pooler_fmt_boxes_level1, )
        _36 = annotate(List[Optional[Tensor]], [inds1])
        _37 = torch.index_put_(output, _36, _35)
        _38 = _7(torch.eq(level_assignments, 3), )
        inds2 = _38[0]
        _39 = annotate(List[Optional[Tensor]], [inds2])
        pooler_fmt_boxes_level2 = torch.index(pooler_fmt_boxes, _39)
        _40 = (_30).forward(x0[3], pooler_fmt_boxes_level2, )
        _41 = annotate(List[Optional[Tensor]], [inds2])
        _42 = torch.index_put_(output, _41, _40)
        _19 = output
      _14 = _19
    return _14

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.ROIAlign
  training : Final[bool] = False
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head, type=MaskRCNNConvUpsampleHead:
class MaskRCNNConvUpsampleHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  vis_period : int
  loss_weight : float
  mask_fcn1 : __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d
  mask_fcn2 : __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d
  mask_fcn3 : __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d
  mask_fcn4 : __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d
  deconv : __torch__.torch.nn.modules.conv.ConvTranspose2d
  deconv_relu : __torch__.torch.nn.modules.activation.ReLU
  predictor : __torch__.detectron2.layers.wrappers.___torch_mangle_42.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead,
    x: Tensor,
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _0 = __torch__.detectron2.modeling.roi_heads.mask_head.mask_rcnn_inference
    x0 = (self).layers(x, )
    _1 = _0(x0, instances, )
    return instances
  def layers(self: __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead,
    x: Tensor) -> Tensor:
    mask_fcn1 = self.mask_fcn1
    mask_fcn2 = self.mask_fcn2
    mask_fcn3 = self.mask_fcn3
    mask_fcn4 = self.mask_fcn4
    deconv = self.deconv
    deconv_relu = self.deconv_relu
    predictor = self.predictor
    x1 = (mask_fcn1).forward(x, )
    x2 = (mask_fcn2).forward(x1, )
    x3 = (mask_fcn3).forward(x2, )
    x4 = (mask_fcn4).forward(x3, )
    x5 = (deconv).forward(x4, None, )
    x6 = (deconv_relu).forward(x5, )
    return (predictor).forward(x6, )
  def __len__(self: __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead) -> int:
    return 7

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : __torch__.torch.nn.modules.activation.ReLU
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    activation = self.activation
    return (activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn1.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : __torch__.torch.nn.modules.activation.ReLU
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    activation = self.activation
    return (activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn2.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : __torch__.torch.nn.modules.activation.ReLU
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    activation = self.activation
    return (activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn3.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : __torch__.torch.nn.modules.activation.ReLU
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding : Final[Tuple[int, int]] = (1, 1)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    activation = self.activation
    return (activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn4.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.deconv, type=ConvTranspose2d:
class ConvTranspose2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 256
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (2, 2)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.conv.ConvTranspose2d,
    input: Tensor,
    output_size: Optional[List[int]]=None) -> Tensor:
    output_padding = (self)._output_padding(input, output_size, [2, 2], [0, 0], [2, 2], 2, [1, 1], )
    weight = self.weight
    bias = self.bias
    _0 = torch.conv_transpose2d(input, weight, bias, [2, 2], [0, 0], output_padding, 1, [1, 1])
    return _0
  def _output_padding(self: __torch__.torch.nn.modules.conv.ConvTranspose2d,
    input: Tensor,
    output_size: Optional[List[int]],
    stride: List[int],
    padding: List[int],
    kernel_size: List[int],
    num_spatial_dims: int,
    dilation: Optional[List[int]]=None) -> List[int]:
    _1 = "ConvTranspose{}D: for {}D input, output_size must have {} or {} elements (got {})"
    _2 = "requested an output size of {}, but valid sizes range from {} to {} (for an input of {})"
    if torch.__is__(output_size, None):
      ret = [0, 0]
    else:
      output_size0 = unchecked_cast(List[int], output_size)
      has_batch_dim = torch.eq(torch.dim(input), torch.add(num_spatial_dims, 2))
      if has_batch_dim:
        num_non_spatial_dims = 2
      else:
        num_non_spatial_dims = 1
      _3 = torch.len(output_size0)
      _4 = torch.add(num_non_spatial_dims, num_spatial_dims)
      if torch.eq(_3, _4):
        output_size2 = torch.slice(output_size0, num_non_spatial_dims)
        output_size1 = output_size2
      else:
        output_size1 = output_size0
      _5 = torch.ne(torch.len(output_size1), num_spatial_dims)
      if _5:
        _6 = torch.dim(input)
        _7 = torch.add(num_non_spatial_dims, num_spatial_dims)
        _8 = torch.format(_1, num_spatial_dims, _6, num_spatial_dims, _7, torch.len(output_size1))
        ops.prim.RaiseException(_8, "builtins.ValueError")
      else:
        pass
      min_sizes = annotate(List[int], [])
      max_sizes = annotate(List[int], [])
      dilation0: Optional[List[int]] = dilation
      for d in range(num_spatial_dims):
        _9 = torch.add(d, num_non_spatial_dims)
        _10 = torch.sub(torch.size(input, _9), 1)
        _11 = torch.sub(torch.mul(_10, stride[d]), torch.mul(2, padding[d]))
        _12 = torch.__isnot__(dilation0, None)
        if _12:
          dilation2 = unchecked_cast(List[int], dilation0)
          _13, dilation1 = dilation2[d], dilation2
        else:
          _13, dilation1 = 1, dilation0
        _14 = torch.mul(_13, torch.sub(kernel_size[d], 1))
        dim_size = torch.add(torch.add(_11, _14), 1)
        _15 = torch.append(min_sizes, dim_size)
        _16 = torch.add(min_sizes[d], stride[d])
        _17 = torch.append(max_sizes, torch.sub(_16, 1))
        dilation0 = dilation1
      for i in range(torch.len(output_size1)):
        size = output_size1[i]
        min_size = min_sizes[i]
        max_size = max_sizes[i]
        if torch.lt(size, min_size):
          _18 = True
        else:
          _18 = torch.gt(size, max_size)
        if _18:
          _19 = torch.slice(torch.size(input), 2)
          _20 = torch.format(_2, output_size1, min_sizes, max_sizes, _19)
          ops.prim.RaiseException(_20, "builtins.ValueError")
        else:
          pass
      res = annotate(List[int], [])
      for d0 in range(num_spatial_dims):
        _21 = torch.sub(output_size1[d0], min_sizes[d0])
        _22 = torch.append(res, _21)
      ret = res
    return ret

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.deconv_relu, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.predictor, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  dilation : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 14
  output_padding : Final[Tuple[int, int]] = (0, 0)
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  in_channels : Final[int] = 256
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_42.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------